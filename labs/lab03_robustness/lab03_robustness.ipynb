{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Why Does My Model Break? ğŸ”\n",
    "\n",
    "**Computer Vision Course**\n",
    "\n",
    "---\n",
    "\n",
    "> *\"A model that only works in perfect conditions is not a real model.\"*\n",
    "\n",
    "In Lab 2 you trained a neural network on MNIST and hit ~97% accuracy. Impressive!\n",
    "\n",
    "But here is a question worth sitting with: **what happens to that accuracy when the images are not perfect?**\n",
    "\n",
    "Real-world images are never perfect. Cameras over-expose. Scans get rotated. Photos are blurry. Lenses add noise. Today you will discover, by experiment, exactly how fragile your model is â€” and begin to understand why.\n",
    "\n",
    "**This lab has two connected parts:**\n",
    "\n",
    "| Part | Activity | Purpose |\n",
    "|------|----------|---------|\n",
    "| **Part 1** | Image Transformations as \"Filters\" | *See* how images can vary |\n",
    "| **Part 2** | MNIST Stress Test | *Measure* what that variation does to your model |\n",
    "\n",
    "By the end you will have answered this question yourself:\n",
    "\n",
    "> *Why do neural networks fail on images they have never seen before?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not os.path.exists('computer-vision'):\n",
    "        !git clone https://github.com/mjck/computer-vision.git\n",
    "    %cd computer-vision/labs/lab03_robustness\n",
    "    sys.path.insert(0, '/content/computer-vision')\n",
    "else:\n",
    "    repo_root = os.path.abspath('../..')\n",
    "    if repo_root not in sys.path:\n",
    "        sys.path.insert(0, repo_root)\n",
    "\n",
    "print(\"âœ… Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy.ndimage import rotate as scipy_rotate\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    from sdx import *\n",
    "    print(\"âœ“ sdx module loaded\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Could not import sdx: {e}\")\n",
    "    raise\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"âœ“ Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1 â€” Image Transformations ğŸ¨\n",
    "\n",
    "Before we stress-test our model, let us first build intuition.\n",
    "\n",
    "In this part you will apply four transformations to a set of images. Each transformation represents something that commonly happens to images in the real world:\n",
    "\n",
    "| Transformation | Real-world cause |\n",
    "|---------------|-----------------|\n",
    "| **Brightness** | Over/under exposure, bad lighting |\n",
    "| **Noise** | Sensor noise, low-quality camera |\n",
    "| **Blur** | Camera shake, out of focus |\n",
    "| **Rotation** | Tilted camera, different viewing angle |\n",
    "\n",
    "Look at the results carefully. Ask yourself: *does this still look like the same image to you?* Remember that answer when you see what it does to your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Sample Images\n",
    "\n",
    "We'll use a small selection of MNIST digits as our test images â€” the same data your model was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True,\n",
    "                               transform=transforms.ToTensor())\n",
    "\n",
    "# Pick one example of each digit 0-9 for visual demos\n",
    "samples = {}\n",
    "for img, label in test_dataset:\n",
    "    if label not in samples:\n",
    "        samples[label] = img.squeeze().numpy()  # (28, 28)\n",
    "    if len(samples) == 10:\n",
    "        break\n",
    "\n",
    "# Display them\n",
    "fig, axes = plt.subplots(1, 10, figsize=(15, 2))\n",
    "for digit, ax in enumerate(axes):\n",
    "    ax.imshow(samples[digit], cmap='gray', vmin=0, vmax=1)\n",
    "    ax.set_title(str(digit), fontsize=12)\n",
    "    ax.axis('off')\n",
    "fig.suptitle(\"Our sample images â€” one of each digit\", fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Images are 28Ã—28 pixels, grayscale, values in [0, 1]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Four Transformations\n",
    "\n",
    "Below are four functions that transform an image. Three of them are already implemented.\n",
    "\n",
    "**Your task:** implement `add_noise()` â€” it is the simplest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Transformation 1: Brightness â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def adjust_brightness(image, factor):\n",
    "    \"\"\"\n",
    "    Multiply every pixel by `factor`.\n",
    "    factor > 1  â†’  brighter\n",
    "    factor < 1  â†’  darker\n",
    "    Values are clipped to stay in [0, 1].\n",
    "\n",
    "    Returns: \n",
    "        Adjusted image, clipped to [0, 1]\n",
    "    \"\"\"\n",
    "    # â”€â”€ your code here â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    pass\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "\n",
    "# â”€â”€ Transformation 2: Noise â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def add_noise(image, std):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise to the image.\n",
    "\n",
    "    Gaussian noise:  each pixel gets a small random value added to it,\n",
    "    drawn from a normal distribution with mean=0 and standard deviation=std.\n",
    "\n",
    "    Args:\n",
    "        image : numpy array, values in [0, 1]\n",
    "        std   : noise strength (try 0.1, 0.3, 0.5)\n",
    "\n",
    "    Returns:\n",
    "        Noisy image, clipped to [0, 1]\n",
    "\n",
    "    TODO: implement this function.\n",
    "    \"\"\"\n",
    "    # â”€â”€ your code here â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    pass\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "\n",
    "# â”€â”€ Transformation 3: Blur â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def apply_blur(image, kernel_size):\n",
    "    \"\"\"\n",
    "    Average each pixel with its neighbours using a square kernel.\n",
    "    kernel_size=1  â†’  no change\n",
    "    kernel_size=5  â†’  visibly blurry\n",
    "    kernel_size=11 â†’  very blurry\n",
    "    \"\"\"\n",
    "    if kernel_size <= 1:\n",
    "        return image.copy()\n",
    "    kernel = np.ones((kernel_size, kernel_size)) / (kernel_size ** 2)\n",
    "    from scipy.ndimage import convolve\n",
    "    return np.clip(convolve(image, kernel), 0, 1)\n",
    "\n",
    "\n",
    "# â”€â”€ Transformation 4: Rotation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def apply_rotation(image, angle_degrees):\n",
    "    \"\"\"\n",
    "    Rotate the image by `angle_degrees` around its centre.\n",
    "    Pixels that fall outside the frame become 0 (black).\n",
    "    TODO: implement this function.\n",
    "    HINT: use scipy.ndimage.rotate\n",
    "    \"\"\"\n",
    "\n",
    "    # â”€â”€ your code here â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    pass\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "\n",
    "print(\"âœ“ Transformation functions defined\")\n",
    "print()\n",
    "print(\"Remember to implement missing functions above before continuing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ Exercise 1 â€” Test Your `add_noise()` Function\n",
    "\n",
    "Run the cell below. If your implementation is correct, the digit should look grainy.\n",
    "If it returns `None` or looks unchanged, go back and fix `add_noise()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test add_noise on the digit \"5\"\n",
    "img = samples[5]\n",
    "\n",
    "noisy = add_noise(img, std=0.3)\n",
    "\n",
    "if noisy is None:\n",
    "    print(\"âŒ add_noise() returned None â€” did you forget the return statement?\")\n",
    "else:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(5, 3))\n",
    "    axes[0].imshow(img,   cmap='gray', vmin=0, vmax=1)\n",
    "    axes[0].set_title(\"Original\")\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(noisy, cmap='gray', vmin=0, vmax=1)\n",
    "    axes[1].set_title(\"Noisy (std=0.3)\")\n",
    "    axes[1].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"âœ“ add_noise() works correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Gallery\n",
    "\n",
    "Let's now see all four transformations applied to digits 3, 7 and 9 â€” three digits that are commonly confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_digits = [3, 7, 9]\n",
    "\n",
    "transformations = [\n",
    "    (\"Original\",         lambda img: img),\n",
    "    (\"Bright (Ã—2.0)\",    lambda img: adjust_brightness(img, 2.0)),\n",
    "    (\"Dark (Ã—0.3)\",      lambda img: adjust_brightness(img, 0.3)),\n",
    "    (\"Noise (0.3)\",      lambda img: add_noise(img, std=0.3)),\n",
    "    (\"Blur (k=7)\",       lambda img: apply_blur(img, kernel_size=7)),\n",
    "    (\"Rotation (30Â°)\",   lambda img: apply_rotation(img, 30)),\n",
    "    (\"Rotation (90Â°)\",   lambda img: apply_rotation(img, 90)),\n",
    "]\n",
    "\n",
    "n_rows = len(demo_digits)\n",
    "n_cols = len(transformations)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 1.8, n_rows * 2.2))\n",
    "fig.suptitle(\"How transformations change digit images\", fontsize=14, y=1.01)\n",
    "\n",
    "for row, digit in enumerate(demo_digits):\n",
    "    for col, (label, fn) in enumerate(transformations):\n",
    "        ax = axes[row][col]\n",
    "        result = fn(samples[digit])\n",
    "        ax.imshow(result, cmap='gray', vmin=0, vmax=1)\n",
    "        ax.axis('off')\n",
    "        if row == 0:\n",
    "            ax.set_title(label, fontsize=8, pad=4)\n",
    "        if col == 0:\n",
    "            ax.set_ylabel(f\"Digit {digit}\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ Exercise 2 â€” Look Before You Measure\n",
    "\n",
    "Before running any model, answer these questions based on what you see in the gallery:\n",
    "\n",
    "1. Which transformation do you think will **hurt the model most?** Why?\n",
    "2. Which transformation do you think the model will **handle best?** Why?\n",
    "3. Is the 90Â° rotated \"7\" still recognisable to you as a human?\n",
    "\n",
    "*(Write your answers as a comment in the cell below. We will revisit them after Part 2.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your predictions (answer all three questions):\n",
    "\n",
    "# 1. The transformation that will hurt the model most:\n",
    "#    ...\n",
    "\n",
    "# 2. The transformation the model will handle best:\n",
    "#    ...\n",
    "\n",
    "# 3. Is the 90Â°-rotated \"7\" still recognisable to you?\n",
    "#    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2 â€” The MNIST Stress Test ğŸ§ª\n",
    "\n",
    "Time to measure what those transformations actually do to a neural network.\n",
    "\n",
    "### Plan\n",
    "1. Train a model on **clean MNIST** (same as Lab 2)\n",
    "2. Evaluate it on **transformed MNIST** â€” same images, different conditions\n",
    "3. Plot the accuracy vs. transformation strength\n",
    "4. Compare results to your predictions from Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "We use the same architecture from Lab 2 â€” a simple network with one hidden layer and ReLU. No changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1  = nn.Linear(28 * 28, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2  = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "\n",
    "model     = SimpleNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model: SimpleNN  |  Parameters: {n_params:,}  |  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on Clean MNIST\n",
    "\n",
    "We train on **unmodified** MNIST for 3 epochs â€” long enough to reach ~97%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='./data', train=True,  download=True,\n",
    "                               transform=transforms.ToTensor())\n",
    "test_dataset  = datasets.MNIST(root='./data', train=False, download=True,\n",
    "                               transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=256, shuffle=False)\n",
    "\n",
    "def train(model, loader, criterion, optimizer, epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total, correct, running_loss = 0, 0, 0.0\n",
    "        bar = tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for imgs, labels in bar:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out  = model(imgs)\n",
    "            loss = criterion(out, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            preds    = out.argmax(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total   += labels.size(0)\n",
    "            bar.set_postfix(loss=f\"{running_loss/(bar.n+1):.3f}\",\n",
    "                            acc=f\"{100*correct/total:.1f}%\")\n",
    "    print(f\"\\nâœ“ Training complete\")\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            preds    = model(imgs).argmax(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total   += labels.size(0)\n",
    "    return 100 * correct / total\n",
    "\n",
    "train(model, train_loader, criterion, optimizer, epochs=3)\n",
    "clean_accuracy = evaluate(model, test_loader)\n",
    "print(f\"\\nğŸ“Š Accuracy on clean test set: {clean_accuracy:.2f}%\")\n",
    "print(\"This is our baseline â€” the best we can do.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Stress Test\n",
    "\n",
    "Now the key experiment.\n",
    "\n",
    "For each transformation, we will:\n",
    "1. Apply it at **several increasing strengths** to the entire test set\n",
    "2. Evaluate the model on each modified version\n",
    "3. Record how accuracy changes\n",
    "\n",
    "The model sees images it has **never encountered in training** â€” just like in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_transform(model, dataset, transform_fn, batch_size=256):\n",
    "    \"\"\"\n",
    "    Apply transform_fn to every image in dataset, then evaluate the model.\n",
    "    transform_fn receives a (28,28) numpy array, returns a (28,28) numpy array.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            # imgs: (B, 1, 28, 28) tensor in [0,1]\n",
    "            imgs_np = imgs.squeeze(1).numpy()  # â†’ (B, 28, 28)\n",
    "\n",
    "            # Apply transform to each image in the batch\n",
    "            transformed = np.stack([transform_fn(img) for img in imgs_np])\n",
    "\n",
    "            # Back to tensor\n",
    "            t = torch.FloatTensor(transformed).unsqueeze(1).to(device)\n",
    "            l = labels.to(device)\n",
    "\n",
    "            preds    = model(t).argmax(1)\n",
    "            correct += (preds == l).sum().item()\n",
    "            total   += l.size(0)\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "\n",
    "print(\"âœ“ evaluate_with_transform() ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running All Experiments\n",
    "\n",
    "This will take 1-2 minutes. Watch the numbers as they appear â€” you may be surprised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "# â”€â”€ Brightness sweep â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"Testing brightness...\")\n",
    "brightness_factors = [0.1, 0.3, 0.5, 0.7, 1.0, 1.5, 2.0, 3.0]\n",
    "results['brightness'] = {\n",
    "    'x'      : brightness_factors,\n",
    "    'xlabel' : 'Brightness factor (1.0 = original)',\n",
    "    'acc'    : []\n",
    "}\n",
    "for f in brightness_factors:\n",
    "    acc = evaluate_with_transform(model, test_dataset,\n",
    "                                  lambda img, f=f: adjust_brightness(img, f))\n",
    "    results['brightness']['acc'].append(acc)\n",
    "    print(f\"  factor={f:.1f}  â†’  {acc:.1f}%\")\n",
    "\n",
    "# â”€â”€ Noise sweep â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nTesting noise...\")\n",
    "noise_stds = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 1.0]\n",
    "results['noise'] = {\n",
    "    'x'      : noise_stds,\n",
    "    'xlabel' : 'Noise standard deviation (0 = no noise)',\n",
    "    'acc'    : []\n",
    "}\n",
    "for s in noise_stds:\n",
    "    acc = evaluate_with_transform(model, test_dataset,\n",
    "                                  lambda img, s=s: add_noise(img, s) if s > 0 else img)\n",
    "    results['noise']['acc'].append(acc)\n",
    "    print(f\"  std={s:.1f}  â†’  {acc:.1f}%\")\n",
    "\n",
    "# â”€â”€ Blur sweep â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nTesting blur...\")\n",
    "kernel_sizes = [1, 3, 5, 7, 9, 11, 15]\n",
    "results['blur'] = {\n",
    "    'x'      : kernel_sizes,\n",
    "    'xlabel' : 'Blur kernel size (1 = no blur)',\n",
    "    'acc'    : []\n",
    "}\n",
    "for k in kernel_sizes:\n",
    "    acc = evaluate_with_transform(model, test_dataset,\n",
    "                                  lambda img, k=k: apply_blur(img, k))\n",
    "    results['blur']['acc'].append(acc)\n",
    "    print(f\"  kernel={k}  â†’  {acc:.1f}%\")\n",
    "\n",
    "# â”€â”€ Rotation sweep â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nTesting rotation...\")\n",
    "angles = [0, 5, 10, 15, 20, 30, 45, 60, 90]\n",
    "results['rotation'] = {\n",
    "    'x'      : angles,\n",
    "    'xlabel' : 'Rotation angle (degrees)',\n",
    "    'acc'    : []\n",
    "}\n",
    "for a in angles:\n",
    "    acc = evaluate_with_transform(model, test_dataset,\n",
    "                                  lambda img, a=a: apply_rotation(img, a))\n",
    "    results['rotation']['acc'].append(acc)\n",
    "    print(f\"  angle={a}Â°  â†’  {acc:.1f}%\")\n",
    "\n",
    "print(\"\\nâœ… All experiments complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results â€” Accuracy vs. Transformation Strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(13, 9))\n",
    "fig.suptitle(\"Model accuracy under image transformations\\n\"\n",
    "             f\"(baseline on clean images: {clean_accuracy:.1f}%)\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "colors = ['#e74c3c', '#e67e22', '#3498db', '#2ecc71']\n",
    "titles = ['Brightness', 'Noise', 'Blur', 'Rotation']\n",
    "keys   = ['brightness', 'noise', 'blur', 'rotation']\n",
    "\n",
    "for ax, key, color, title in zip(axes.flat, keys, colors, titles):\n",
    "    r = results[key]\n",
    "    ax.plot(r['x'], r['acc'], 'o-', color=color, linewidth=2, markersize=6)\n",
    "    ax.axhline(y=clean_accuracy, color='gray', linestyle='--',\n",
    "               linewidth=1.5, label=f\"Baseline ({clean_accuracy:.1f}%)\")\n",
    "    ax.axhline(y=10, color='black', linestyle=':', linewidth=1,\n",
    "               label=\"Random chance (10%)\")\n",
    "    ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel(r['xlabel'], fontsize=10)\n",
    "    ax.set_ylabel(\"Test Accuracy (%)\", fontsize=10)\n",
    "    ax.set_ylim(0, 105)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Annotate the baseline value on the line\n",
    "    for xi, yi in zip(r['x'], r['acc']):\n",
    "        if yi < clean_accuracy - 5:   # only annotate drops > 5%\n",
    "            ax.annotate(f\"{yi:.0f}%\", (xi, yi),\n",
    "                        textcoords=\"offset points\", xytext=(0, 8),\n",
    "                        fontsize=8, color=color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 55)\n",
    "print(f\"{'Transformation':<20} {'Best':>8} {'Worst':>8} {'Drop':>8}\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "for key, title in zip(keys, titles):\n",
    "    r      = results[key]\n",
    "    best   = max(r['acc'])\n",
    "    worst  = min(r['acc'])\n",
    "    drop   = best - worst\n",
    "    print(f\"{title:<20} {best:>7.1f}%  {worst:>7.1f}%  {drop:>6.1f}%\")\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(f\"{'Baseline (clean)':<20} {clean_accuracy:>7.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ Exercise 3 â€” What Does the Model Actually See?\n",
    "\n",
    "Let's look at the images the model gets confused by. Pick one transformation and one strength level that caused a big accuracy drop. Visualise some examples where the model is **wrong**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Experiment here â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Change these to explore different failures:\n",
    "TRANSFORM_NAME = \"rotation\"     # 'brightness', 'noise', 'blur', 'rotation'\n",
    "TRANSFORM_FN   = lambda img: apply_rotation(img, angle_degrees=30)\n",
    "\n",
    "# â”€â”€ Find mistakes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model.eval()\n",
    "wrong_images, wrong_preds, wrong_true = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in DataLoader(test_dataset, batch_size=256):\n",
    "        imgs_np     = imgs.squeeze(1).numpy()\n",
    "        transformed = np.stack([TRANSFORM_FN(img) for img in imgs_np])\n",
    "        t           = torch.FloatTensor(transformed).unsqueeze(1).to(device)\n",
    "        preds       = model(t).argmax(1).cpu()\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            if preds[i] != labels[i]:\n",
    "                wrong_images.append(transformed[i])\n",
    "                wrong_preds.append(preds[i].item())\n",
    "                wrong_true.append(labels[i].item())\n",
    "            if len(wrong_images) >= 20:\n",
    "                break\n",
    "        if len(wrong_images) >= 20:\n",
    "            break\n",
    "\n",
    "# â”€â”€ Display â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "n_show = min(20, len(wrong_images))\n",
    "fig, axes = plt.subplots(2, 10, figsize=(16, 4))\n",
    "fig.suptitle(f\"Examples where the model is WRONG  |  Transform: {TRANSFORM_NAME}\",\n",
    "             fontsize=12, fontweight='bold')\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < n_show:\n",
    "        ax.imshow(wrong_images[i], cmap='gray', vmin=0, vmax=1)\n",
    "        ax.set_title(f\"True: {wrong_true[i]}\\nPred: {wrong_preds[i]}\",\n",
    "                     fontsize=8,\n",
    "                     color='red' if wrong_true[i] != wrong_preds[i] else 'green')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Showing {n_show} mistakes out of {len(wrong_images)} found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ Exercise 4 â€” Revisit Your Predictions\n",
    "\n",
    "Go back to Exercise 2 and compare your predictions to the actual results.\n",
    "\n",
    "Answer these questions in the cell below:\n",
    "\n",
    "1. Were your predictions correct? What surprised you?\n",
    "2. Which digit suffers most from rotation? Look at the gallery from Part 1 and think about why.\n",
    "3. The model was trained on upright digits. What would you need to do so it handles rotated images correctly?\n",
    "4. *(Bonus)* Can you think of a medical imaging scenario where this fragility would be dangerous?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your reflections:\n",
    "\n",
    "# 1. Were your predictions correct?\n",
    "#    ...\n",
    "\n",
    "# 2. Which digit suffers most from rotation, and why?\n",
    "#    ...\n",
    "\n",
    "# 3. What would you need to do to handle rotated images?\n",
    "#    ...\n",
    "\n",
    "# 4. (Bonus) A medical imaging scenario where this fragility is dangerous:\n",
    "#    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3 â€” A Glimpse of the Fix ğŸ’¡\n",
    "\n",
    "You have seen the problem. Now let's peek at a solution.\n",
    "\n",
    "The idea is simple: if the model never saw rotated images during training, of course it fails on them. So â€” **train it on rotated images too**.\n",
    "\n",
    "This is called **data augmentation**: artificially expanding your training set by showing the model transformed versions of images it already has.\n",
    "\n",
    "Let's test whether it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Train with augmentation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "augmented_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=30),     # Random rotation Â±30Â°\n",
    "    transforms.RandomAffine(degrees=0,\n",
    "                            translate=(0.1, 0.1)),  # Slight shift\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "aug_train_dataset = datasets.MNIST(root='./data', train=True, download=True,\n",
    "                                    transform=augmented_transform)\n",
    "aug_train_loader  = DataLoader(aug_train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Fresh model with same architecture\n",
    "model_aug     = SimpleNN().to(device)\n",
    "optimizer_aug = optim.Adam(model_aug.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Training with rotation augmentation (3 epochs)...\")\n",
    "train(model_aug, aug_train_loader, criterion, optimizer_aug, epochs=3)\n",
    "\n",
    "clean_aug = evaluate(model_aug, test_loader)\n",
    "print(f\"\\nğŸ“Š Augmented model â€” accuracy on clean images: {clean_aug:.2f}%\")\n",
    "print(f\"   Original model  â€” accuracy on clean images: {clean_accuracy:.2f}%\")\n",
    "print(\"\\n(Small drop on clean images is normal â€” the model is now more general)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare both models on rotated images\n",
    "print(\"Comparing original vs augmented model on rotated images:\\n\")\n",
    "print(f\"{'Angle':>8} | {'Original':>10} | {'Augmented':>10} | {'Improvement':>12}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for angle in [0, 10, 20, 30, 45, 60, 90]:\n",
    "    fn  = lambda img, a=angle: apply_rotation(img, a)\n",
    "    acc_orig = evaluate_with_transform(model,     test_dataset, fn)\n",
    "    acc_aug  = evaluate_with_transform(model_aug, test_dataset, fn)\n",
    "    diff     = acc_aug - acc_orig\n",
    "    bar      = \"â–²\" * max(0, int(diff / 3))\n",
    "    print(f\"{angle:>6}Â°  | {acc_orig:>9.1f}% | {acc_aug:>9.1f}% |\"\n",
    "          f\" {diff:>+8.1f}%  {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "angles_plot = [0, 10, 20, 30, 45, 60, 90]\n",
    "acc_orig_list = [evaluate_with_transform(model,     test_dataset,\n",
    "                                          lambda img, a=a: apply_rotation(img, a))\n",
    "                 for a in angles_plot]\n",
    "acc_aug_list  = [evaluate_with_transform(model_aug, test_dataset,\n",
    "                                          lambda img, a=a: apply_rotation(img, a))\n",
    "                 for a in angles_plot]\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(angles_plot, acc_orig_list, 'o-', color='#e74c3c',\n",
    "        linewidth=2, markersize=7, label='Original model')\n",
    "ax.plot(angles_plot, acc_aug_list,  's-', color='#2ecc71',\n",
    "        linewidth=2, markersize=7, label='Augmented model')\n",
    "ax.axhline(y=10, color='black', linestyle=':', linewidth=1, label='Random chance')\n",
    "ax.set_title(\"Accuracy vs Rotation Angle\", fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel(\"Rotation angle (degrees)\")\n",
    "ax.set_ylabel(\"Accuracy (%)\")\n",
    "ax.set_ylim(0, 105)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "improvements = [a - b for a, b in zip(acc_aug_list, acc_orig_list)]\n",
    "colors_bar   = ['#27ae60' if x >= 0 else '#e74c3c' for x in improvements]\n",
    "ax.bar(angles_plot, improvements, color=colors_bar, width=6, alpha=0.85)\n",
    "ax.axhline(y=0, color='black', linewidth=1)\n",
    "ax.set_title(\"Improvement from Augmentation\", fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel(\"Rotation angle (degrees)\")\n",
    "ax.set_ylabel(\"Accuracy gain (%)\")\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Wrapping Up ğŸ¯\n",
    "\n",
    "### What you discovered today\n",
    "\n",
    "You started with a model achieving ~97% accuracy and found that:\n",
    "\n",
    "- A **small rotation** (30Â°) can drop accuracy dramatically\n",
    "- **Noise** degrades performance gradually but consistently\n",
    "- **Brightness extremes** can almost completely fool the model\n",
    "- **Training with augmentation** largely solves the rotation problem\n",
    "\n",
    "### The deeper question\n",
    "\n",
    "These transformations are *physically meaningful*:\n",
    "\n",
    "| You applied | Next week, you will understand |\n",
    "|-------------|-------------------------------|\n",
    "| Brightness  | **Light and exposure** â€” how cameras collect photons |\n",
    "| Noise       | **Sensor noise** â€” electronics in camera sensors |\n",
    "| Blur        | **Optics** â€” lenses, focus, depth of field |\n",
    "| Rotation    | **Viewpoint** â€” camera angle, object pose |\n",
    "\n",
    "**Image formation is not just theory.** It directly explains why your model fails and what you need to do about it.\n",
    "\n",
    "### âœï¸ Exercise 5 â€” Final Reflection\n",
    "\n",
    "Answer in the cell below:\n",
    "\n",
    "1. Which result from today surprised you the most?\n",
    "2. Name one real-world application where image variation (brightness, blur, rotation) is a major problem.\n",
    "3. Data augmentation helped with rotation. Do you think it would also help with brightness changes? With blur? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final reflections:\n",
    "\n",
    "# 1. What surprised you most?\n",
    "#    ...\n",
    "\n",
    "# 2. A real application where image variation is a major problem:\n",
    "#    ...\n",
    "\n",
    "# 3. Would augmentation help with brightness? With blur?\n",
    "#    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“‹ Submission Checklist\n",
    "\n",
    "Before submitting this notebook, make sure:\n",
    "\n",
    "- [ ] `adjust_brightness()`, `add_noise()`, `apply_rotation()` are implemented and working (Exercise 1)\n",
    "- [ ] Exercise 2 predictions are written\n",
    "- [ ] The full stress test ran (all four accuracy curves are plotted)\n",
    "- [ ] You explored at least one failure case (Exercise 3)\n",
    "- [ ] Exercises 4 and 5 are answered\n",
    "- [ ] All cells have been executed in order\n",
    "\n",
    "**Grading:**\n",
    "- Implementation of transformation functions: Adequate\n",
    "- Stress test results present: Adequate\n",
    "- Exercises 2, 4, 5 answered thoughtfully: Above adequate\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
