{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: The Very Basic Basics of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Computer Vision Course - Lab 2: Neural Networks\n",
    "\n",
    "This cell sets up the environment.\n",
    "Works automatically for both local and Google Colab!\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Computer Vision - Lab 2 Setup\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"\\nðŸ”µ Running on Google Colab\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Clone repository if not already present\n",
    "    if not os.path.exists('computer-vision'):\n",
    "        print(\"ðŸ“¥ Cloning repository...\")\n",
    "        !git clone https://github.com/mjck/computer-vision.git\n",
    "        print(\"âœ“ Repository cloned successfully\")\n",
    "    else:\n",
    "        print(\"âœ“ Repository already exists\")\n",
    "    \n",
    "    # Navigate to lab directory\n",
    "    %cd computer-vision/labs/lab02_neural_networks\n",
    "    print(f\"âœ“ Current directory: {os.getcwd()}\")\n",
    "    \n",
    "    # Add repository root to Python path\n",
    "    sys.path.insert(0, '/content/computer-vision')\n",
    "    print(\"âœ“ Python path configured\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(\"ðŸŸ¢ Colab setup complete!\\n\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nðŸŸ¢ Running locally\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"âœ“ Current directory: {os.getcwd()}\")\n",
    "    \n",
    "    # Add repository root to Python path\n",
    "    repo_root = os.path.abspath('../..')\n",
    "    if repo_root not in sys.path:\n",
    "        sys.path.insert(0, repo_root)\n",
    "    print(f\"âœ“ Repository root: {repo_root}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(\"ðŸŸ¢ Local setup complete!\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"âœ… Environment ready!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import course utilities\n",
    "from sdx import *\n",
    "\n",
    "# Set device (GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Displaying the MNIST Dataset\n",
    "\n",
    "The [MNIST dataset](http://yann.lecun.com/exdb/mnist/) is a famous dataset of handwritten digits.\n",
    "\n",
    "Machine learning datasets consist basically of input-output pairs. Since our interest is computer vision, the inputs are naturally images, while the outputs depend on the problem. In this notebook, we consider a *classification* problem: each output is a *label* that indicates the *category* to which the image belongs.\n",
    "\n",
    "It is recommended to separate these pairs in a set of *training data* and a set of *testing data,* because the goal is obtaining a model that works well with data that was *not* used for training.\n",
    "\n",
    "The MNIST dataset already provides a separation. If that was not the case, we would have to obtain one. *(we will do this in later classes)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load MNIST dataset\n",
    "# PyTorch downloads to a 'data' folder by default\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Extract images and labels as numpy arrays for visualization\n",
    "train_images = train_dataset.data.numpy()\n",
    "train_labels = train_dataset.targets.numpy()\n",
    "test_images = test_dataset.data.numpy()\n",
    "test_labels = test_dataset.targets.numpy()\n",
    "\n",
    "print(f\"Training set: {len(train_images)} images\")\n",
    "print(f\"Test set: {len(test_images)} images\")\n",
    "print(f\"Image shape: {train_images[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `train_images` is an array of 60000 images that should be used as training data. Below, we see the tenth image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_imshow(train_images[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `train_labels` is an array of 60000 integers, which are the respective labels of these images. These integers were obtained manually, so they are reliable to use as groundtruth. Below, we see the label of the tenth image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `test_images` is an array of 10000 images that should be used as testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_imshow(test_images[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the variable `test_labels` is an array of 10000 integers, which are its respective labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show more than one image at once, we can call the `cv_gridshow` function, passing the array and the parameters of a slice. Below, we show the 25 images that correspond to the `train_images[10:35]` slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_gridshow(train_images, start=10, stop=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to show the labels alongside the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_gridshow(train_images, start=10, stop=35, labels=train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default value for `start` is `0` and the default value for `stop` is `9`. This is important because the processing happens in Google servers but the rendering still happens in your machine. If you accidentally tried to show 60000 images at once, your browser would die horribly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_gridshow(train_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Neural Networks\n",
    "\n",
    "Let's build a first neural network. More accurately, let's *demonstrate the foundations of building a neural network.* The actual model we will build is so trivial that we can't really call it an actual neural network, since it will have no hidden layers. *(we will properly discuss the concept of hidden layers in later classes)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network in PyTorch\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Flatten layer is implicit in forward() method\n",
    "        # Dense layer: 28*28 = 784 inputs -> 10 outputs\n",
    "        self.fc = nn.Linear(28 * 28, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Flatten the input: (batch, 28, 28) -> (batch, 784)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Pass through dense layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create the model and move to device\n",
    "model = SimpleNN().to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above defines a model with the following structure:\n",
    "\n",
    "1. **Input**: Images with shape `(28, 28)` - the MNIST images have 28 rows, 28 columns, and only 1 color channel, since they are gray level images. *(as seen in the previous class)*\n",
    "\n",
    "2. **Flatten**: The `view` operation in the `forward` method reshapes the 2D input tensors into 1D tensors by simply concatenating the rows. This is necessary because the next layer expects 1D tensors.\n",
    "\n",
    "3. **Dense (Linear) layer**: Transforms each 1D tensor of size 784 (28Ã—28) into another 1D tensor of size `10`. This is the only layer with trainable parameters. *(we will properly discuss how this layer works in later classes)*\n",
    "\n",
    "The last layer must return tensors of size `10` because the *problem itself* has an output of size `10`: for each of the ten categories, the model returns the likelihood of the image belonging to it. The classification is made by simply choosing the one with greatest likelihood.\n",
    "\n",
    "After building a model, let's look at its parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(\"\\nModel architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Show parameter shapes\n",
    "print(\"\\nParameter details:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name:20s} {str(param.shape):20s} {param.numel():>10,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, before using the model, we must set up the training process. We need to specify:\n",
    "\n",
    "- A **loss function** that the training process will attempt to minimize\n",
    "- An **optimizer** that will update the model parameters\n",
    "\n",
    "The code below specifies *Cross-Entropy Loss* as the loss function and *Stochastic Gradient Descent (SGD)* as the optimizer. These choices are standard for classification problems with integer labels, which is precisely the case of MNIST.\n",
    "\n",
    "We will not delve much into these specific details in this course. If you are interested, we recommend taking the *Machine Learning* elective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Loss function:\", criterion)\n",
    "print(\"Optimizer:\", optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now we can train the model. In PyTorch, we need to explicitly write the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=1):\n",
    "    model.train()  # Set to training mode\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Progress bar\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for images, labels in pbar:\n",
    "            # Move data to device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{running_loss/(pbar.n+1):.4f}',\n",
    "                'acc': f'{100*correct/total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}, Accuracy = {epoch_acc:.2f}%\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "After training, we evaluate the model on the test set to see how well it generalizes to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # No gradients needed for evaluation\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate\n",
    "test_accuracy = evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A confusion matrix helps visualize which digits the model confuses with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(model, test_loader):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=range(10), yticklabels=range(10))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Model with a Hidden Layer\n",
    "\n",
    "Now let's add a hidden layer to improve performance. This creates a proper neural network with multiple layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedNN(nn.Module):\n",
    "    def __init__(self, hidden_size=128):\n",
    "        super(ImprovedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, hidden_size)  # First layer\n",
    "        self.fc2 = nn.Linear(hidden_size, 10)        # Output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc1(x)            # First layer\n",
    "        x = self.fc2(x)            # Output layer (no activation yet)\n",
    "        return x\n",
    "\n",
    "# Create improved model\n",
    "model = ImprovedNN(hidden_size=128).to(device)\n",
    "\n",
    "# Setup optimizer and loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the improved model\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=1)\n",
    "\n",
    "# Evaluate\n",
    "evaluate_model(model, test_loader)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with Neural Network Hyperparameters\n",
    "\n",
    "**Questions to explore:**\n",
    "\n",
    "* What happens when you add more layers?\n",
    "* What happens when you use smaller hidden layers?\n",
    "* What happens when you use larger hidden layers?\n",
    "* What happens when you add `nn.ReLU()` activation after the first layer?\n",
    "\n",
    "Try it below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding ReLU Activation\n",
    "\n",
    "Let's add a non-linear activation function (ReLU) after the hidden layer. This allows the network to learn more complex patterns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNWithReLU(nn.Module):\n",
    "    def __init__(self, hidden_size=128):\n",
    "        super(NNWithReLU, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, hidden_size)\n",
    "        self.relu = nn.ReLU()  # Non-linear activation!\n",
    "        self.fc2 = nn.Linear(hidden_size, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc1(x)            # First layer\n",
    "        x = self.relu(x)           # Activation function\n",
    "        x = self.fc2(x)            # Output layer\n",
    "        return x\n",
    "\n",
    "# Create model with ReLU\n",
    "model = NNWithReLU(hidden_size=128).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with ReLU\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=1)\n",
    "\n",
    "# Evaluate\n",
    "evaluate_model(model, test_loader)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn!\n",
    "\n",
    "Now it's your turn! Do whatever you want in an attempt to increase the accuracy.\n",
    "\n",
    "You can check the [PyTorch documentation](https://pytorch.org/docs/stable/nn.html), but I won't help you. Just try to do your best by yourself.\n",
    "\n",
    "**Ideas to try:**\n",
    "- Add more hidden layers\n",
    "- Change the hidden layer size\n",
    "- Try different activation functions (ReLU, LeakyReLU, Tanh)\n",
    "- Add dropout: `nn.Dropout(0.2)`\n",
    "- Change the learning rate\n",
    "- Train for more epochs\n",
    "- Try a different optimizer (Adam, RMSprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom model here!\n",
    "class CustomNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 397)  # Try different sizes!\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(397, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create your custom model\n",
    "model = CustomNN().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your custom model\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=1)\n",
    "\n",
    "# Evaluate\n",
    "evaluate_model(model, test_loader)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "- âœ… How to load the MNIST dataset in PyTorch\n",
    "- âœ… How to build neural networks using `nn.Module`\n",
    "- âœ… How to define layers (Linear/Dense layers)\n",
    "- âœ… How to set up loss functions and optimizers\n",
    "- âœ… How to write a training loop\n",
    "- âœ… How to evaluate model performance\n",
    "- âœ… How to visualize results with confusion matrices\n",
    "- âœ… The importance of activation functions (ReLU)\n",
    "- âœ… How hyperparameters affect model performance\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Experiment with different architectures\n",
    "- Try training for multiple epochs\n",
    "- Learn about convolutional layers (upcoming classes)\n",
    "- Explore data augmentation\n",
    "- Try other datasets"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
